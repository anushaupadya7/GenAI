# 🔢 What Are Tokens & Tokenization in AI Models?  

This repository explains — in simple words — **what tokens are**, **how tokenization works**, and **how your text is converted into numbers** that Large Language Models (LLMs) like ChatGPT, Gemini, and Claude can actually understand.  

---

## 💡 What is a Token?

A **token** is the smallest piece of text an AI model understands.  
It can be:
- A **word** → `"apple"`  
- A **part of a word** → `"ap"`, `"ple"`  
- A **punctuation mark** → `"!"`, `"."`  
- Or even **spaces** and **special symbols**  

The model doesn’t see letters or words —  
it only sees **numbers** (called *token IDs*) that represent those pieces of text.

Example:

"I love AI!"
↓
["I", " love", " AI", "!"]
↓
[40, 400, 1256, 0]


Here each number corresponds to a token in the model’s internal dictionary.  

---

## 🧩 What is Tokenization?

**Tokenization** is the process of splitting your text into tokens before giving it to the model.  
Think of it as the “language preprocessing” step.

| Text | Tokens | Token IDs |
|------|---------|------------|
| "I love AI!" | ["I", " love", " AI", "!"] | [40, 400, 1256, 0] |

Every model (GPT, BERT, LLaMA, Claude) has its **own tokenizer** because they were trained differently.

| Model | Tokenization Method |
|--------|----------------------|
| GPT-3 / GPT-4 | Byte Pair Encoding (BPE) |
| BERT | WordPiece |
| LLaMA / Gemini | SentencePiece |

So the *same sentence* may split differently depending on the model.

---

## 📘 What is a Vocabulary?

A **vocabulary** is the complete set of tokens that a model knows.  
It’s like the model’s **dictionary**, mapping every possible token to a unique integer ID.

Example:
| Token | Token ID |
|--------|-----------|
| "I" | 40 |
| " love" | 400 |
| " AI" | 1256 |
| "!" | 0 |

Each token in this vocabulary has a unique **Token ID**, and together they form the **vocabulary space**.

---

## 🔢 Vocabulary Size — The Limit of What a Model Knows

- The **vocabulary size** determines **how many unique tokens** a model can understand.  
- Each token ID is an integer between **0 and (vocab_size - 1)**.  
- The larger the vocabulary, the more nuanced and language-rich the model can be — but it also becomes computationally heavier.

| Model | Vocabulary Size | Example |
|--------|------------------|----------|
| GPT-2 | 50,257 tokens | Based on English internet text |
| GPT-4 (cl100k_base) | 100,000 tokens | Multilingual & code-friendly |
| BERT-base | 30,522 tokens | English-focused |
| LLaMA-2 | ~32,000 tokens | Uses SentencePiece |

So yes —  
🧠 **Token IDs depend directly on the vocabulary size.**  
If the vocabulary size = 100,000, token IDs range from **0 → 99,999**.

---

## ⚙️ How Text Becomes Numbers (Step-by-Step)

Let’s take an example sentence:  


### 🪄 Step 1: Tokenization


### 🔢 Step 2: Token–ID Mapping  
Each token is looked up in the model’s **vocabulary file** — a huge dictionary of token → ID pairs.

| Token | Token ID |
|--------|-----------|
| "Transform" | 6782 |
| "ers" | 345 |
| " are" | 423 |
| " amazing" | 1123 |
| "!" | 0 |

So the sentence becomes:

[6782, 345, 423, 1123, 0]


### 🧬 Step 3: Embedding Lookup  
Each ID is converted into a **vector** — a list of numbers that represents its meaning:

6782 → [0.21, -0.33, 0.45, ...]



These embeddings are then passed through transformer layers to learn **context and relationships** between words.

---

## 🔍 Visualize Tokenization

You can **see tokens in action** at:  
🎨 [https://tiktokenizer.vercel.app/](https://tiktokenizer.vercel.app/)

![Example of tokenization](./token.png?WT.mc_id=academic-105485-koreyst)

Try typing your own sentence (like `"ChatGPT loves math!"`)  
and you’ll see:
- How the text splits into tokens  
- Their token IDs  
- The total number of tokens  

---

## 🧠 Why Tokens Matter

Tokens determine:
- ✅ **How much text the model can handle** (context limit)  
- 💰 **How much API usage costs** (you pay per token)  
- ⚡ **How fast the model responds**  
- 🧩 **How well prompts fit into the context window**

For example:
- GPT-4-Turbo can handle **up to 128,000 tokens** in a single prompt.
- Roughly, **1 token ≈ 4 characters of English text**.

---

## 💻 Quick Example (Using Python)

```python
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")
text = "I love learning about transformers!"
tokens = enc.encode(text)

print("Tokens:", tokens)
print("Decoded:", enc.decode(tokens))
print("Number of tokens:", len(tokens))
```

Tokens: [40, 400, 1832, 552, 8251, 0]
Decoded: I love learning about transformers!
Number of tokens: 6


| Concept                                     | Description                                              |
| ------------------------------------------- | -------------------------------------------------------- |
| **Token**                                   | Smallest unit of text the model understands              |
| **Tokenization**                            | Process of splitting text into tokens                    |
| **Token ID**                                | Numeric representation of each token                     |
| **Embedding**                               | Vector representation of a token used by the transformer |
| **Different Models → Different Tokenizers** | Because they’re trained on different vocabularies        |

