# 🔢 What Are Tokens & Tokenization in AI Models?  

This repository explains — in simple words — **what tokens are**, **how tokenization works**, and **how your text is converted into numbers** that Large Language Models (LLMs) like ChatGPT, Gemini, and Claude can actually understand.  

---

## 💡 What is a Token?

A **token** is the smallest piece of text an AI model understands.  
It can be:
- A **word** → `"apple"`  
- A **part of a word** → `"ap"`, `"ple"`  
- A **punctuation mark** → `"!"`, `"."`  
- Or even **spaces** and **special symbols**  

The model doesn’t see letters or words —  
it only sees **numbers** (called *token IDs*) that represent those pieces of text.

Example:

"I love AI!"
↓
["I", " love", " AI", "!"]
↓
[40, 400, 1256, 0]


Here each number corresponds to a token in the model’s internal dictionary.  

---

## 🧩 What is Tokenization?

**Tokenization** is the process of splitting your text into tokens before giving it to the model.  
Think of it as the “language preprocessing” step.

| Text | Tokens | Token IDs |
|------|---------|------------|
| "I love AI!" | ["I", " love", " AI", "!"] | [40, 400, 1256, 0] |

Every model (GPT, BERT, LLaMA, Claude) has its **own tokenizer** because they were trained differently.

| Model | Tokenization Method |
|--------|----------------------|
| GPT-3 / GPT-4 | Byte Pair Encoding (BPE) |
| BERT | WordPiece |
| LLaMA / Gemini | SentencePiece |

So the *same sentence* may split differently depending on the model.

---

## ⚙️ How Text Becomes Numbers (Step-by-Step)

Let’s take an example sentence:  


### 🪄 Step 1: Tokenization


### 🔢 Step 2: Token–ID Mapping  
Each token is looked up in the model’s **vocabulary file** — a huge dictionary of token → ID pairs.

| Token | Token ID |
|--------|-----------|
| "Transform" | 6782 |
| "ers" | 345 |
| " are" | 423 |
| " amazing" | 1123 |
| "!" | 0 |

So the sentence becomes:

[6782, 345, 423, 1123, 0]


### 🧬 Step 3: Embedding Lookup  
Each ID is converted into a **vector** — a list of numbers that represents its meaning:

6782 → [0.21, -0.33, 0.45, ...]



These embeddings are then passed through transformer layers to learn **context and relationships** between words.

---

## 🔍 Visualize Tokenization

You can **see tokens in action** at:  
🎨 [https://tiktokenizer.vercel.app/](https://tiktokenizer.vercel.app/)

![Example of tokenization](./token.png?WT.mc_id=academic-105485-koreyst)

Try typing your own sentence (like `"ChatGPT loves math!"`)  
and you’ll see:
- How the text splits into tokens  
- Their token IDs  
- The total number of tokens  

---

## 🧠 Why Tokens Matter

Tokens determine:
- ✅ **How much text the model can handle** (context limit)  
- 💰 **How much API usage costs** (you pay per token)  
- ⚡ **How fast the model responds**  
- 🧩 **How well prompts fit into the context window**

For example:
- GPT-4-Turbo can handle **up to 128,000 tokens** in a single prompt.
- Roughly, **1 token ≈ 4 characters of English text**.

---

## 💻 Quick Example (Using Python)

```python
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")
text = "I love learning about transformers!"
tokens = enc.encode(text)

print("Tokens:", tokens)
print("Decoded:", enc.decode(tokens))
print("Number of tokens:", len(tokens))
```

Tokens: [40, 400, 1832, 552, 8251, 0]
Decoded: I love learning about transformers!
Number of tokens: 6


| Concept                                     | Description                                              |
| ------------------------------------------- | -------------------------------------------------------- |
| **Token**                                   | Smallest unit of text the model understands              |
| **Tokenization**                            | Process of splitting text into tokens                    |
| **Token ID**                                | Numeric representation of each token                     |
| **Embedding**                               | Vector representation of a token used by the transformer |
| **Different Models → Different Tokenizers** | Because they’re trained on different vocabularies        |

