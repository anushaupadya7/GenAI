# ğŸ§© Vector Embeddings

Vector **embeddings** are numerical representations of data â€” such as **text, images, or audio** â€” that capture their **meaning** and **relationships** in a continuous vector space.  
They allow machines to compare **semantic similarity** instead of literal string equality.

---

## ğŸ“˜ Example: Text Embeddings

| Word | Embedding (example 3D vector) |
|------|-------------------------------|
| â€œcatâ€ | [0.21, 0.91, 0.33] |
| â€œdogâ€ | [0.20, 0.85, 0.31] |
| â€œcarâ€ | [0.88, 0.12, 0.44] |

Here, the **distance** between â€œcatâ€ and â€œdogâ€ is smaller than between â€œcatâ€ and â€œcar,â€  
meaning the model understands cats and dogs are semantically closer.

---

## âš™ï¸ How Are Embeddings Generated?

Embeddings are learned by **neural networks** during training.  

1. **Input:** â€œI love machine learningâ€  
2. **Tokenization:** â†’ `["I", "love", "machine", "learning"]`  
3. **Mapping:** Each token â†’ numerical vector using an **embedding layer**  
4. **Training:** The model adjusts vectors so similar meanings lie closer together

---

## ğŸ§  Where Are Embeddings Used?

| Application | Purpose |
|--------------|----------|
| **Search Engines** | Find semantically similar documents (not just exact matches) |
| **Chatbots / RAG Systems** | Retrieve relevant info based on meaning |
| **Recommendation Systems** | Suggest similar items or products |
| **Clustering & Classification** | Group similar data points together |

---

## ğŸ”¢ Mathematical Intuition

An embedding is a **vector** in an *n-dimensional space*.  
Similarity is often measured using **cosine similarity**:

\[
\text{similarity}(A, B) = \frac{A \cdot B}{||A|| \, ||B||}
\]

A value closer to `1` â†’ more similar meaning.

---

## ğŸ§­ Why Are Embeddings Powerful?

- Convert **discrete** data â†’ **continuous** vector space  
- Capture **semantic meaning**  
- Enable arithmetic on meanings  

Example:  
\[
\text{vector}("king") - \text{vector}("man") + \text{vector}("woman") â‰ˆ \text{vector}("queen")
\]

---

## ğŸ—‚ï¸ Types of Embeddings

| Type | Example | Description |
|------|----------|-------------|
| **Word Embeddings** | Word2Vec, GloVe | Represent words in fixed-dimensional space |
| **Sentence Embeddings** | Sentence-BERT | Represent full sentences or paragraphs |
| **Document Embeddings** | doc2vec, OpenAI text-embedding models | Represent entire documents |
| **Multimodal Embeddings** | CLIP, DALL-E | Link text and images in same vector space |

---

## ğŸš€ In the Context of LLMs

Large Language Models (LLMs) like GPT use embeddings internally.

In **Retrieval-Augmented Generation (RAG)**:

1. Text data â†’ embeddings  
2. User query â†’ embedding  
3. Compare similarity using cosine distance  
4. Retrieve top results to provide context for model responses

---

## ğŸ“ Vocabulary & Token IDs Connection

- Each token in a modelâ€™s **vocabulary** has a **token ID**  
- The **embedding matrix** maps each token ID to its vector  

If:
- Vocab size = 50,000  
- Embedding dimension = 768  

Then the embedding matrix = **50,000 Ã— 768**

âœ… Larger vocab â†’ more expressive  
âš ï¸ But heavier model

---

## ğŸ§© Visualization Example

